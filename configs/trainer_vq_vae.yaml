# Data parameters
joint_size: 1659
# Window size -1 means that the whole sequence is used
window_size: -1

# Wandb parameters
wandb_api_key: <insert your key here>

# Training parameters
batch_size: 1
max_epochs: 4000
save_every_n_epochs: 10
model_hyperparameters:
  learning_rate_scheduler: 'static'
  learning_rate: 0.0002
  betas:
    - 0.9
    - 0.99
  L: 2
  codebook_size: 512
  quantizer_decay: 0.99
  skels_dim: 1659
  embedding_dim: 512
  is_focus_hand_mode: False
  ratio_for_hand: 0.9

# Paths
train_data_path: '/home/cpgang/data/extracted_phoenix_full/train/train.skels'
# train_data_tensor_path: ''
val_data_path: '/home/cpgang/data/extracted_phoenix_full/train/train.skels'
# val_data_tensor_path: ''
is_data_has_timestamp: False
save_weight_path: '/home/cpgang/PP/trained_model_ckpt/vq_vae_model_cont2.pth'
log_folder_name: 'vqvae_train_gcp_test_cont2'
wandb_save_dir: '/mnt/disks/general_backup/wandb_log'
resume_weight_path: '/home/cpgang/PP/trained_model_ckpt/vq_vae_model_cont.pth'
pretrained_weight_path: ''

# Is data normalized before training
normalize_data: True
