{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from T2M_GPT_lightning.models.text2sign import Text2Sign\n",
    "from T2M_GPT_lightning.dataset.toy_vq_vae_dataset import ToyDataset as ToyVQVAEDataset\n",
    "from T2M_GPT_lightning.dataset.toy_t2m_trans_dataset import ToyDataset as ToyT2MTransDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE model\n",
    "VQ_VAE_MODEL_WEIGHT_PATH = \"../logs/vq_vae/version_8/checkpoints/epoch=1999-step=10000.ckpt\"\n",
    "VQ_VAE_MODEL_CONFIG_PATH = \"./configs/vq_vae_model_config.yaml\"\n",
    "# CLIP model\n",
    "CLIP_MODEL = \"ViT-B/32\"\n",
    "# GPT model\n",
    "T2M_TRANS_MODEL_WEIGHT_PATH = \"./weights/t2m_trans_model.pth\"\n",
    "T2M_TRANS_MODEL_CONIFG_PATH = \"./configs/t2m_trans_model_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sign = Text2Sign.from_path(\n",
    "    VQ_VAE_MODEL_WEIGHT_PATH,\n",
    "    VQ_VAE_MODEL_CONFIG_PATH,\n",
    "    CLIP_MODEL,\n",
    "    T2M_TRANS_MODEL_WEIGHT_PATH,\n",
    "    T2M_TRANS_MODEL_CONIFG_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_vae_dataset = ToyVQVAEDataset(\"../data/toy_data/train.skels\", 150, 100, 32)\n",
    "t2m_trans_dataset = ToyT2MTransDataset(\n",
    "    text_to_sign.clip_model,\n",
    "    text_to_sign.vq_vae_model,\n",
    "    \"../data/toy_data/train.text\",\n",
    "    \"../data/toy_data/train.skels\",\n",
    "    100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"auch am tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen .\"\n",
      "--------------------------------------------------\n",
      "Skeleton: tensor([[0.4154, 0.2883, 0.3328,  ..., 0.4363, 0.6263, 0.5929],\n",
      "        [0.4155, 0.2875, 0.3333,  ..., 0.4393, 0.6241, 0.5934],\n",
      "        [0.4123, 0.2885, 0.3333,  ..., 0.4338, 0.6260, 0.6037],\n",
      "        ...,\n",
      "        [0.4846, 0.2343, 0.4674,  ..., 0.5611, 0.9324, 0.7510],\n",
      "        [0.4793, 0.2324, 0.4676,  ..., 0.5613, 0.9443, 0.7521],\n",
      "        [0.4763, 0.2313, 0.4667,  ..., 0.5614, 0.9495, 0.7484]])\n",
      "Skeleton shape: torch.Size([185, 150])\n",
      "--------------------------------------------------\n",
      "Skeleton indices: tensor([16, 24, 42, 43, 43, 42, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 60, 60, 24, 24, 24, 42, 16, 43, 43, 43, 43, 42, 16, 43, 43, 43,\n",
      "        42, 42, 43, 43, 43, 16, 42, 24, 24, 24, 42, 24, 24, 24, 24, 24, 24, 16,\n",
      "        43, 42, 24, 24, 24, 24, 42, 42, 24, 24, 24, 60, 60, 16, 43, 43, 43, 42,\n",
      "        24, 60, 60, 60, 24, 24, 24, 42, 43, 43, 42, 42, 16, 16, 42, 24, 24, 60,\n",
      "        60, 27, 64], device='mps:0')\n",
      "Skeleton indices shape: torch.Size([93])\n",
      "--------------------------------------------------\n",
      "Reconstructed skeleton: tensor([[[0.4329, 0.2409, 0.4200,  ..., 0.5943, 0.5185, 0.6171],\n",
      "         [0.4370, 0.2422, 0.4351,  ..., 0.6008, 0.5215, 0.6342],\n",
      "         [0.4444, 0.2369, 0.4430,  ..., 0.5936, 0.5558, 0.6201],\n",
      "         ...,\n",
      "         [0.4408, 0.2643, 0.4178,  ..., 0.5535, 0.7950, 0.7058],\n",
      "         [0.4391, 0.2606, 0.4135,  ..., 0.5574, 0.8311, 0.6967],\n",
      "         [0.4297, 0.2525, 0.4112,  ..., 0.5656, 0.8159, 0.6971]]],\n",
      "       device='mps:0', grad_fn=<PermuteBackward0>)\n",
      "Reconstructed skeleton shape: torch.Size([1, 184, 150])\n",
      "--------------------------------------------------\n",
      "Indices prediction: tensor([[ 35,   6,  82, 127,  38,  38,  15,  15,  15,  15,  15, 120,  15,  15,\n",
      "          15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15, 120,  15,  15,\n",
      "          15,  15,  15,  15,  15,   6, 127,  15,  15,  15,  15,  15, 120,  15,\n",
      "          15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,\n",
      "          15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,\n",
      "          15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,  15,\n",
      "          15,  15,  15,  15,  15,  15,  15,  15]], device='mps:0')\n",
      "Indices prediction shape: torch.Size([1, 92])\n",
      "--------------------------------------------------\n",
      "Sign prediction: tensor([[[0.4382, 0.2478, 0.4427,  ..., 0.5676, 0.7425, 0.7056],\n",
      "         [0.4281, 0.2509, 0.4388,  ..., 0.5822, 0.7775, 0.7037],\n",
      "         [0.4277, 0.2555, 0.4340,  ..., 0.5854, 0.7953, 0.7065],\n",
      "         ...,\n",
      "         [0.4214, 0.2626, 0.4168,  ..., 0.5721, 0.7882, 0.7151],\n",
      "         [0.4178, 0.2559, 0.4134,  ..., 0.5734, 0.7810, 0.7158],\n",
      "         [0.4261, 0.2563, 0.4054,  ..., 0.5725, 0.7655, 0.7237]]],\n",
      "       device='mps:0', grad_fn=<PermuteBackward0>)\n",
      "Sign prediction shape: torch.Size([1, 184, 150])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_index = 3\n",
    "\n",
    "text = t2m_trans_dataset.texts[data_index]\n",
    "print(f'Text: \"{text}\"')\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel = vq_vae_dataset.get_full_sequences_by_idx(data_index)\n",
    "print(f\"Skeleton: {skel}\")\n",
    "print(f\"Skeleton shape: {skel.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel_indices = t2m_trans_dataset[data_index][1]\n",
    "print(f\"Skeleton indices: {skel_indices}\")\n",
    "print(f\"Skeleton indices shape: {skel_indices.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel_reconstructed = t2m_trans_dataset.vq_vae_model.decode_indices(skel_indices[:-1].unsqueeze(0))\n",
    "print(f\"Reconstructed skeleton: {skel_reconstructed}\")\n",
    "print(f\"Reconstructed skeleton shape: {skel_reconstructed.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "indices_prediction = text_to_sign.text_to_indices(text)\n",
    "print(f\"Indices prediction: {indices_prediction}\")\n",
    "print(f\"Indices prediction shape: {indices_prediction.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sign_prediction = text_to_sign.text_to_skels(text)\n",
    "print(f\"Sign prediction: {sign_prediction}\")\n",
    "print(f\"Sign prediction shape: {sign_prediction.shape}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "\n",
    "# Plot a video given a tensor of joints, a file path, video name and references/sequence ID\n",
    "def plot_video(joints, file_path, video_name, references=None, skip_frames=1, sequence_ID=None):\n",
    "    # Create video template\n",
    "    FPS = 25 // skip_frames\n",
    "    video_file = file_path + \"/{}.mp4\".format(video_name.split(\".\")[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "    if references is None:\n",
    "        video = cv2.VideoWriter(video_file, fourcc, float(FPS), (650, 650), True)\n",
    "    elif references is not None:\n",
    "        video = cv2.VideoWriter(video_file, fourcc, float(FPS), (1300, 650), True)  # Long\n",
    "\n",
    "    num_frames = 0\n",
    "\n",
    "    for j, frame_joints in enumerate(joints):\n",
    "\n",
    "        # Reached padding\n",
    "        if PAD_TOKEN in frame_joints:\n",
    "            continue\n",
    "\n",
    "        # Initialise frame of white\n",
    "        frame = np.ones((650, 650, 3), np.uint8) * 255\n",
    "\n",
    "        # Cut off the percent_tok, multiply by 3 to restore joint size\n",
    "        # TODO - Remove the *3 if the joints weren't divided by 3 in data creation\n",
    "        frame_joints = frame_joints[:] * 3\n",
    "\n",
    "        # Reduce the frame joints down to 2D for visualisation - Frame joints 2d shape is (48,2)\n",
    "        frame_joints_2d = np.reshape(frame_joints, (50, 3))[:, :2]\n",
    "        # Draw the frame given 2D joints\n",
    "        draw_frame_2D(frame, frame_joints_2d)\n",
    "\n",
    "        cv2.putText(frame, \"Predicted Sign Pose\", (180, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # If reference is provided, create and concatenate on the end\n",
    "        if references is not None:\n",
    "            # Extract the reference joints\n",
    "            ref_joints = references[j]\n",
    "            # Initialise frame of white\n",
    "            ref_frame = np.ones((650, 650, 3), np.uint8) * 255\n",
    "\n",
    "            # Cut off the percent_tok and multiply each joint by 3 (as was reduced in training files)\n",
    "            ref_joints = ref_joints[:-1] * 3\n",
    "\n",
    "            # Reduce the frame joints down to 2D- Frame joints 2d shape is (48,2)\n",
    "            ref_joints_2d = np.reshape(ref_joints, (50, 3))[:, :2]\n",
    "\n",
    "            # Draw these joints on the frame\n",
    "            draw_frame_2D(ref_frame, ref_joints_2d)\n",
    "\n",
    "            cv2.putText(ref_frame, \"Ground Truth Pose\", (190, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "            frame = np.concatenate((frame, ref_frame), axis=1)\n",
    "\n",
    "            sequence_ID_write = \"Sequence ID: \" + sequence_ID.split(\"/\")[-1]\n",
    "            cv2.putText(frame, sequence_ID_write, (700, 635), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "        # Write the video frame\n",
    "        video.write(frame)\n",
    "        num_frames += 1\n",
    "    # Release the video\n",
    "    video.release()\n",
    "\n",
    "\n",
    "# This is the format of the 3D data, outputted from the Inverse Kinematics model\n",
    "def getSkeletalModelStructure():\n",
    "    # Definition of skeleton model structure:\n",
    "    #   The structure is an n-tuple of:\n",
    "    #\n",
    "    #   (index of a start point, index of an end point, index of a bone)\n",
    "    #\n",
    "    #   E.g., this simple skeletal model\n",
    "    #\n",
    "    #             (0)\n",
    "    #              |\n",
    "    #              |\n",
    "    #              0\n",
    "    #              |\n",
    "    #              |\n",
    "    #     (2)--1--(1)--1--(3)\n",
    "    #      |               |\n",
    "    #      |               |\n",
    "    #      2               2\n",
    "    #      |               |\n",
    "    #      |               |\n",
    "    #     (4)             (5)\n",
    "    #\n",
    "    #   has this structure:\n",
    "    #\n",
    "    #   (\n",
    "    #     (0, 1, 0),\n",
    "    #     (1, 2, 1),\n",
    "    #     (1, 3, 1),\n",
    "    #     (2, 4, 2),\n",
    "    #     (3, 5, 2),\n",
    "    #   )\n",
    "    #\n",
    "    #  Warning 1: The structure has to be a tree.\n",
    "    #  Warning 2: The order isn't random. The order is from a root to lists.\n",
    "    #\n",
    "\n",
    "    return (\n",
    "        # head\n",
    "        (0, 1, 0),\n",
    "        # left shoulder\n",
    "        (1, 2, 1),\n",
    "        # left arm\n",
    "        (2, 3, 2),\n",
    "        # (3, 4, 3),\n",
    "        # Changed to avoid wrist, go straight to hands\n",
    "        (3, 29, 3),\n",
    "        # right shoulder\n",
    "        (1, 5, 1),\n",
    "        # right arm\n",
    "        (5, 6, 2),\n",
    "        # (6, 7, 3),\n",
    "        # Changed to avoid wrist, go straight to hands\n",
    "        (6, 8, 3),\n",
    "        # left hand - wrist\n",
    "        # (7, 8, 4),\n",
    "        # left hand - palm\n",
    "        (8, 9, 5),\n",
    "        (8, 13, 9),\n",
    "        (8, 17, 13),\n",
    "        (8, 21, 17),\n",
    "        (8, 25, 21),\n",
    "        # left hand - 1st finger\n",
    "        (9, 10, 6),\n",
    "        (10, 11, 7),\n",
    "        (11, 12, 8),\n",
    "        # left hand - 2nd finger\n",
    "        (13, 14, 10),\n",
    "        (14, 15, 11),\n",
    "        (15, 16, 12),\n",
    "        # left hand - 3rd finger\n",
    "        (17, 18, 14),\n",
    "        (18, 19, 15),\n",
    "        (19, 20, 16),\n",
    "        # left hand - 4th finger\n",
    "        (21, 22, 18),\n",
    "        (22, 23, 19),\n",
    "        (23, 24, 20),\n",
    "        # left hand - 5th finger\n",
    "        (25, 26, 22),\n",
    "        (26, 27, 23),\n",
    "        (27, 28, 24),\n",
    "        # right hand - wrist\n",
    "        # (4, 29, 4),\n",
    "        # right hand - palm\n",
    "        (29, 30, 5),\n",
    "        (29, 34, 9),\n",
    "        (29, 38, 13),\n",
    "        (29, 42, 17),\n",
    "        (29, 46, 21),\n",
    "        # right hand - 1st finger\n",
    "        (30, 31, 6),\n",
    "        (31, 32, 7),\n",
    "        (32, 33, 8),\n",
    "        # right hand - 2nd finger\n",
    "        (34, 35, 10),\n",
    "        (35, 36, 11),\n",
    "        (36, 37, 12),\n",
    "        # right hand - 3rd finger\n",
    "        (38, 39, 14),\n",
    "        (39, 40, 15),\n",
    "        (40, 41, 16),\n",
    "        # right hand - 4th finger\n",
    "        (42, 43, 18),\n",
    "        (43, 44, 19),\n",
    "        (44, 45, 20),\n",
    "        # right hand - 5th finger\n",
    "        (46, 47, 22),\n",
    "        (47, 48, 23),\n",
    "        (48, 49, 24),\n",
    "    )\n",
    "\n",
    "\n",
    "# Draw a line between two points, if they are positive points\n",
    "def draw_line(im, joint1, joint2, c=(0, 0, 255), t=1, width=3):\n",
    "    thresh = -100\n",
    "    if joint1[0] > thresh and joint1[1] > thresh and joint2[0] > thresh and joint2[1] > thresh:\n",
    "\n",
    "        center = (int((joint1[0] + joint2[0]) / 2), int((joint1[1] + joint2[1]) / 2))\n",
    "\n",
    "        length = int(math.sqrt(((joint1[0] - joint2[0]) ** 2) + ((joint1[1] - joint2[1]) ** 2)) / 2)\n",
    "\n",
    "        angle = math.degrees(math.atan2((joint1[0] - joint2[0]), (joint1[1] - joint2[1])))\n",
    "\n",
    "        cv2.ellipse(im, center, (width, length), -angle, 0.0, 360.0, c, -1)\n",
    "\n",
    "\n",
    "# Draw the frame given 2D joints that are in the Inverse Kinematics format\n",
    "def draw_frame_2D(frame, joints):\n",
    "    # Line to be between the stacked\n",
    "    draw_line(frame, [1, 650], [1, 1], c=(0, 0, 0), t=1, width=1)\n",
    "    # Give an offset to center the skeleton around\n",
    "    offset = [350, 250]\n",
    "    # offset = [0, 0]\n",
    "\n",
    "    # Get the skeleton structure details of each bone, and size\n",
    "    skeleton = getSkeletalModelStructure()\n",
    "    skeleton = np.array(skeleton)\n",
    "\n",
    "    number = skeleton.shape[0]\n",
    "\n",
    "    # Increase the size and position of the joints\n",
    "    joints = joints * 10 * 12 * 2\n",
    "    joints = joints + np.ones((50, 2)) * offset\n",
    "\n",
    "    # Loop through each of the bone structures, and plot the bone\n",
    "    for j in range(number):\n",
    "\n",
    "        c = get_bone_colour(skeleton, j)\n",
    "\n",
    "        draw_line(\n",
    "            frame,\n",
    "            [joints[skeleton[j, 0]][0], joints[skeleton[j, 0]][1]],\n",
    "            [joints[skeleton[j, 1]][0], joints[skeleton[j, 1]][1]],\n",
    "            c=c,\n",
    "            t=1,\n",
    "            width=1,\n",
    "        )\n",
    "\n",
    "\n",
    "# get bone colour given index\n",
    "def get_bone_colour(skeleton, j):\n",
    "    bone = skeleton[j, 2]\n",
    "\n",
    "    if bone == 0:  # head\n",
    "        c = (0, 153, 0)\n",
    "    elif bone == 1:  # Shoulder\n",
    "        c = (0, 0, 255)\n",
    "\n",
    "    elif bone == 2 and skeleton[j, 1] == 3:  # left arm\n",
    "        c = (0, 102, 204)\n",
    "    elif bone == 3 and skeleton[j, 0] == 3:  # left lower arm\n",
    "        c = (0, 204, 204)\n",
    "\n",
    "    elif bone == 2 and skeleton[j, 1] == 6:  # right arm\n",
    "        c = (0, 153, 0)\n",
    "    elif bone == 3 and skeleton[j, 0] == 6:  # right lower arm\n",
    "        c = (0, 204, 0)\n",
    "\n",
    "    # Hands\n",
    "    elif bone in [5, 6, 7, 8]:\n",
    "        c = (0, 0, 255)\n",
    "    elif bone in [9, 10, 11, 12]:\n",
    "        c = (51, 255, 51)\n",
    "    elif bone in [13, 14, 15, 16]:\n",
    "        c = (255, 0, 0)\n",
    "    elif bone in [17, 18, 19, 20]:\n",
    "        c = (204, 153, 255)\n",
    "    elif bone in [21, 22, 23, 24]:\n",
    "        c = (51, 255, 255)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "# Apply DTW to the produced sequence, so it can be visually compared to the reference sequence\n",
    "def alter_DTW_timing(pred_seq, ref_seq):\n",
    "\n",
    "    # Define a cost function\n",
    "    euclidean_norm = lambda x, y: np.sum(np.abs(x - y))\n",
    "\n",
    "    # Cut the reference down to the max count value\n",
    "    _, ref_max_idx = torch.max(ref_seq[:, -1], 0)\n",
    "    if ref_max_idx == 0:\n",
    "        ref_max_idx += 1\n",
    "    # Cut down frames by counter\n",
    "    ref_seq = ref_seq[:ref_max_idx, :].cpu().numpy()\n",
    "\n",
    "    # Cut the hypothesis down to the max count value\n",
    "    _, hyp_max_idx = torch.max(pred_seq[:, -1], 0)\n",
    "    if hyp_max_idx == 0:\n",
    "        hyp_max_idx += 1\n",
    "    # Cut down frames by counter\n",
    "    pred_seq = pred_seq[:hyp_max_idx, :].cpu().numpy()\n",
    "\n",
    "    # Run DTW on the reference and predicted sequence\n",
    "    d, cost_matrix, acc_cost_matrix, path = dtw(ref_seq[:, :-1], pred_seq[:, :-1], dist=euclidean_norm)\n",
    "\n",
    "    # Normalise the dtw cost by sequence length\n",
    "    d = d / acc_cost_matrix.shape[0]\n",
    "\n",
    "    # Initialise new sequence\n",
    "    new_pred_seq = np.zeros_like(ref_seq)\n",
    "    # j tracks the position in the reference sequence\n",
    "    j = 0\n",
    "    skips = 0\n",
    "    squeeze_frames = []\n",
    "    for i, pred_num in enumerate(path[0]):\n",
    "\n",
    "        if i == len(path[0]) - 1:\n",
    "            break\n",
    "\n",
    "        if path[1][i] == path[1][i + 1]:\n",
    "            skips += 1\n",
    "\n",
    "        # If a double coming up\n",
    "        if path[0][i] == path[0][i + 1]:\n",
    "            squeeze_frames.append(pred_seq[i - skips])\n",
    "            j += 1\n",
    "        # Just finished a double\n",
    "        elif path[0][i] == path[0][i - 1]:\n",
    "            new_pred_seq[pred_num] = avg_frames(squeeze_frames)\n",
    "            squeeze_frames = []\n",
    "        else:\n",
    "            new_pred_seq[pred_num] = pred_seq[i - skips]\n",
    "\n",
    "    return new_pred_seq, ref_seq, d\n",
    "\n",
    "\n",
    "# Find the average of the given frames\n",
    "def avg_frames(frames):\n",
    "    frames_sum = np.zeros_like(frames[0])\n",
    "    for frame in frames:\n",
    "        frames_sum += frame\n",
    "\n",
    "    avg_frame = frames_sum / len(frames)\n",
    "    return avg_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = vq_vae_dataset.min_value\n",
    "max_value = vq_vae_dataset.max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to original values\n",
    "# converted_skel = (skel * (max_value - min_value)) + min_value\n",
    "# plot_video(skel, \".\", \"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auch am tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2m_trans_dataset.texts[data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/160h3py942nb4pbbgy0dhtz40000gn/T/ipykernel_27290/2480778442.py:237: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  joints = joints + np.ones((50, 2)) * offset\n"
     ]
    }
   ],
   "source": [
    "converted_skel_reconstructed = skel_reconstructed[0].cpu().detach()\n",
    "converted_skel_reconstructed = (converted_skel_reconstructed * (max_value - min_value)) + min_value\n",
    "\n",
    "plot_video(converted_skel_reconstructed, \".\", \"predicted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
