{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from T2M_GPT_lightning.models.text2sign import Text2Sign\n",
    "from T2M_GPT_lightning.dataset.toy_vq_vae_dataset import ToyDataset as ToyVQVAEDataset\n",
    "from T2M_GPT_lightning.dataset.toy_t2m_trans_dataset import ToyDataset as ToyT2MTransDataset\n",
    "\n",
    "from capstone_utils.plot_skeletons import plot_skeletons_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE model\n",
    "VQ_VAE_MODEL_WEIGHT_PATH = \"../logs/vq_vae/version_9/checkpoints/epoch=1999-step=10000.ckpt\"\n",
    "VQ_VAE_MODEL_CONFIG_PATH = \"./configs/vq_vae_model_config.yaml\"\n",
    "# CLIP model\n",
    "CLIP_MODEL = \"ViT-B/32\"\n",
    "# GPT model\n",
    "T2M_TRANS_MODEL_WEIGHT_PATH = \"./weights/t2m_trans_model.pth\"\n",
    "T2M_TRANS_MODEL_CONIFG_PATH = \"./configs/t2m_trans_model_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sign = Text2Sign.from_path(\n",
    "    VQ_VAE_MODEL_WEIGHT_PATH,\n",
    "    VQ_VAE_MODEL_CONFIG_PATH,\n",
    "    CLIP_MODEL,\n",
    "    T2M_TRANS_MODEL_WEIGHT_PATH,\n",
    "    T2M_TRANS_MODEL_CONIFG_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_vae_dataset = ToyVQVAEDataset(\"../data/toy_data/train.skels\", 150, 100, 32)\n",
    "t2m_trans_dataset = ToyT2MTransDataset(\n",
    "    text_to_sign.clip_model,\n",
    "    text_to_sign.vq_vae_model,\n",
    "    \"../data/toy_data/train.text\",\n",
    "    \"../data/toy_data/train.skels\",\n",
    "    100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"auch am tag gibt es verbreitet zum teil kr√§ftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen .\"\n",
      "--------------------------------------------------\n",
      "Skeleton: tensor([[0.4154, 0.2883, 0.3328,  ..., 0.4363, 0.6263, 0.5929],\n",
      "        [0.4155, 0.2875, 0.3333,  ..., 0.4393, 0.6241, 0.5934],\n",
      "        [0.4123, 0.2885, 0.3333,  ..., 0.4338, 0.6260, 0.6037],\n",
      "        ...,\n",
      "        [0.4846, 0.2343, 0.4674,  ..., 0.5611, 0.9324, 0.7510],\n",
      "        [0.4793, 0.2324, 0.4676,  ..., 0.5613, 0.9443, 0.7521],\n",
      "        [0.4763, 0.2313, 0.4667,  ..., 0.5614, 0.9495, 0.7484]])\n",
      "Skeleton shape: torch.Size([185, 150])\n",
      "--------------------------------------------------\n",
      "Skeleton indices: tensor([196, 262,  69, 411,  69,  69,  69,  69,  69, 411,  69,  69,  69, 404,\n",
      "         14, 153, 157,  14,  69,  14,  69, 411,  69, 251,  69,  69, 509, 101,\n",
      "        411,  69,  69,  69, 196, 325,  14,  69, 196, 411,  69, 123, 274, 503,\n",
      "         14, 411, 196, 196, 512], device='mps:0')\n",
      "Skeleton indices shape: torch.Size([47])\n",
      "--------------------------------------------------\n",
      "Reconstructed skeleton: tensor([[[0.4314, 0.2355, 0.3931,  ..., 0.4714, 0.6966, 0.6105],\n",
      "         [0.4591, 0.2478, 0.4069,  ..., 0.4957, 0.7639, 0.6328],\n",
      "         [0.4612, 0.2515, 0.4060,  ..., 0.5200, 0.7695, 0.6277],\n",
      "         ...,\n",
      "         [0.3770, 0.2399, 0.3881,  ..., 0.5257, 0.6633, 0.7110],\n",
      "         [0.3663, 0.2506, 0.3790,  ..., 0.5319, 0.6550, 0.7083],\n",
      "         [0.3665, 0.2506, 0.3784,  ..., 0.5344, 0.6409, 0.7117]]],\n",
      "       device='mps:0', grad_fn=<PermuteBackward0>)\n",
      "Reconstructed skeleton shape: torch.Size([1, 184, 150])\n",
      "--------------------------------------------------\n",
      "Indices prediction: tensor([[16, 24, 42, 43, 43, 42, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "         24, 24, 60, 60, 24, 24, 24, 42, 16, 43, 43, 43, 43, 43, 43, 43, 43, 42,\n",
      "         42, 42, 43, 43, 43, 16, 42, 24, 24, 24, 42, 24, 24, 24, 24, 24, 24, 16,\n",
      "         43, 42, 24, 24, 24, 24, 42, 42, 24, 24, 24, 60, 60, 16, 43, 43, 43, 42,\n",
      "         24, 60, 60, 60, 24, 24, 24, 42, 43, 43, 42, 42, 16, 16, 42, 24, 24, 60,\n",
      "         60, 27]])\n",
      "Indices prediction shape: torch.Size([1, 92])\n",
      "--------------------------------------------------\n",
      "Sign prediction: tensor([[[0.4394, 0.2018, 0.4633,  ..., 0.7057, 0.4059, 0.8249],\n",
      "         [0.4239, 0.1983, 0.4574,  ..., 0.6855, 0.3897, 0.8182],\n",
      "         [0.4049, 0.1963, 0.4426,  ..., 0.6918, 0.3714, 0.8263],\n",
      "         ...,\n",
      "         [0.4010, 0.2443, 0.3927,  ..., 0.5772, 0.5602, 0.6758],\n",
      "         [0.3896, 0.2467, 0.3807,  ..., 0.5692, 0.5740, 0.6851],\n",
      "         [0.3758, 0.2443, 0.3664,  ..., 0.5422, 0.5800, 0.6710]]],\n",
      "       device='mps:0', grad_fn=<PermuteBackward0>)\n",
      "Sign prediction shape: torch.Size([1, 368, 150])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_index = 3\n",
    "\n",
    "text = t2m_trans_dataset.texts[data_index]\n",
    "print(f'Text: \"{text}\"')\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel = vq_vae_dataset.get_full_sequences_by_idx(data_index)\n",
    "print(f\"Skeleton: {skel}\")\n",
    "print(f\"Skeleton shape: {skel.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel_indices = t2m_trans_dataset[data_index][1]\n",
    "print(f\"Skeleton indices: {skel_indices}\")\n",
    "print(f\"Skeleton indices shape: {skel_indices.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel_reconstructed = text_to_sign.vq_vae_model.decode_indices(skel_indices[:-1].unsqueeze(0))\n",
    "print(f\"Reconstructed skeleton: {skel_reconstructed}\")\n",
    "print(f\"Reconstructed skeleton shape: {skel_reconstructed.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "indices_prediction = text_to_sign.text_to_indices(text)\n",
    "print(f\"Indices prediction: {indices_prediction}\")\n",
    "print(f\"Indices prediction shape: {indices_prediction.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sign_prediction = text_to_sign.text_to_skels(text)\n",
    "print(f\"Sign prediction: {sign_prediction}\")\n",
    "print(f\"Sign prediction shape: {sign_prediction.shape}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = vq_vae_dataset.min_value\n",
    "max_value = vq_vae_dataset.max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to original values\n",
    "converted_skel = (skel * (max_value - min_value)) + min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 185 is out of bounds for dimension 0 with size 185",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m converted_sign_prediction \u001b[38;5;241m=\u001b[39m sign_prediction[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     14\u001b[0m converted_sign_prediction \u001b[38;5;241m=\u001b[39m (converted_sign_prediction \u001b[38;5;241m*\u001b[39m (max_value \u001b[38;5;241m-\u001b[39m min_value)) \u001b[38;5;241m+\u001b[39m min_value\n\u001b[0;32m---> 15\u001b[0m \u001b[43mplot_skeletons_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_sign_prediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFOLDER_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_index\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Capstone/src/capstone_utils/plot_skeletons.py:47\u001b[0m, in \u001b[0;36mplot_skeletons_video\u001b[0;34m(joints, file_path, video_name, references, skip_frames, sequence_ID)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# If reference is provided, create and concatenate on the end\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m references \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Extract the reference joints\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     ref_joints \u001b[38;5;241m=\u001b[39m \u001b[43mreferences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Initialise frame of white\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     ref_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m650\u001b[39m, \u001b[38;5;241m650\u001b[39m, \u001b[38;5;241m3\u001b[39m), np\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 185 is out of bounds for dimension 0 with size 185"
     ]
    }
   ],
   "source": [
    "FOLDER_NAME = 'cb=512'\n",
    "if FOLDER_NAME:\n",
    "    import os\n",
    "\n",
    "    os.makedirs(FOLDER_NAME, exist_ok=True)\n",
    "else:\n",
    "    FOLDER_NAME = \".\"\n",
    "\n",
    "converted_skel_reconstructed = skel_reconstructed[0].cpu().detach()\n",
    "converted_skel_reconstructed = (converted_skel_reconstructed * (max_value - min_value)) + min_value\n",
    "plot_skeletons_video(converted_skel_reconstructed, FOLDER_NAME, \"reconstruction\", skel, 1, f\"{data_index}\")\n",
    "\n",
    "converted_sign_prediction = sign_prediction[0].cpu().detach()\n",
    "converted_sign_prediction = (converted_sign_prediction * (max_value - min_value)) + min_value\n",
    "plot_skeletons_video(converted_sign_prediction, FOLDER_NAME, \"prediction\", skel, 1, f\"{data_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
