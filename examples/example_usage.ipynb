{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from T2M_GPT_lightning.models.text2sign import Text2Sign\n",
    "from T2M_GPT_lightning.dataset.toy_vq_vae_dataset import ToyDataset as ToyVQVAEDataset\n",
    "from T2M_GPT_lightning.dataset.toy_t2m_trans_dataset import ToyDataset as ToyT2MTransDataset\n",
    "\n",
    "from capstone_utils.plot_skeletons import plot_skeletons_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE model\n",
    "VQ_VAE_MODEL_WEIGHT_PATH = \"../logs/vq_vae_finetune/version_0/checkpoints/epoch=999-step=5000.ckpt\"\n",
    "VQ_VAE_MODEL_CONFIG_PATH = \"./configs/vq_vae_model_config.yaml\"\n",
    "# CLIP model\n",
    "CLIP_MODEL = \"ViT-B/32\"\n",
    "# GPT model\n",
    "T2M_TRANS_MODEL_WEIGHT_PATH = \"./weights/t2m_trans_model.pth\"\n",
    "T2M_TRANS_MODEL_CONIFG_PATH = \"./configs/t2m_trans_model_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sign = Text2Sign.from_path(\n",
    "    VQ_VAE_MODEL_WEIGHT_PATH,\n",
    "    VQ_VAE_MODEL_CONFIG_PATH,\n",
    "    CLIP_MODEL,\n",
    "    T2M_TRANS_MODEL_WEIGHT_PATH,\n",
    "    T2M_TRANS_MODEL_CONIFG_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_vae_dataset = ToyVQVAEDataset(\"../data/toy_data/train.skels\", 150, 100, 32)\n",
    "t2m_trans_dataset = ToyT2MTransDataset(\n",
    "    text_to_sign.clip_model,\n",
    "    text_to_sign.vq_vae_model,\n",
    "    \"../data/toy_data/train.text\",\n",
    "    \"../data/toy_data/train.skels\",\n",
    "    100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"auch am tag gibt es verbreitet zum teil kr√§ftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen .\"\n",
      "--------------------------------------------------\n",
      "Skeleton: tensor([[0.4154, 0.2883, 0.3328,  ..., 0.4363, 0.6263, 0.5929],\n",
      "        [0.4155, 0.2875, 0.3333,  ..., 0.4393, 0.6241, 0.5934],\n",
      "        [0.4123, 0.2885, 0.3333,  ..., 0.4338, 0.6260, 0.6037],\n",
      "        ...,\n",
      "        [0.4846, 0.2343, 0.4674,  ..., 0.5611, 0.9324, 0.7510],\n",
      "        [0.4793, 0.2324, 0.4676,  ..., 0.5613, 0.9443, 0.7521],\n",
      "        [0.4763, 0.2313, 0.4667,  ..., 0.5614, 0.9495, 0.7484]])\n",
      "Skeleton shape: torch.Size([185, 150])\n",
      "--------------------------------------------------\n",
      "Skeleton indices: tensor([411,  52,  69, 411, 326,  52,  69,  69,  69, 411,  69,  69,  69, 326,\n",
      "         14,  69,  14,  14,  69,  14,  69,  69,  69,  14,  69,  69,  52,  67,\n",
      "        411,  69,  69,  69, 411,  69,  14,  69, 411, 411, 479, 326,  14, 326,\n",
      "         69, 411, 411, 411, 512], device='mps:0')\n",
      "Skeleton indices shape: torch.Size([47])\n",
      "--------------------------------------------------\n",
      "Reconstructed skeleton: tensor([[[0.4573, 0.2421, 0.4166,  ..., 0.5112, 0.6589, 0.6154],\n",
      "         [0.4814, 0.2500, 0.4316,  ..., 0.5154, 0.6889, 0.6199],\n",
      "         [0.4955, 0.2493, 0.4518,  ..., 0.5090, 0.7062, 0.6235],\n",
      "         ...,\n",
      "         [0.4102, 0.2698, 0.3900,  ..., 0.6189, 0.6497, 0.8066],\n",
      "         [0.4068, 0.2634, 0.3855,  ..., 0.6051, 0.6509, 0.7853],\n",
      "         [0.4037, 0.2490, 0.3888,  ..., 0.5846, 0.6547, 0.7411]]],\n",
      "       device='mps:0', grad_fn=<PermuteBackward0>)\n",
      "Reconstructed skeleton shape: torch.Size([1, 184, 150])\n",
      "--------------------------------------------------\n",
      "Indices prediction: tensor([[16, 24, 42, 43, 43, 42, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "         24, 24, 60, 60, 24, 24, 24, 42, 16, 43, 43, 43, 43, 43, 43, 43, 43, 42,\n",
      "         42, 42, 43, 43, 43, 16, 42, 24, 24, 24, 42, 24, 24, 24, 24, 24, 24, 16,\n",
      "         43, 42, 24, 24, 24, 24, 42, 42, 24, 24, 24, 60, 60, 16, 43, 43, 43, 42,\n",
      "         24, 60, 60, 60, 24, 24, 24, 42, 43, 43, 42, 42, 16, 16, 42, 24, 24, 60,\n",
      "         60, 27]])\n",
      "Indices prediction shape: torch.Size([1, 92])\n",
      "--------------------------------------------------\n",
      "Sign prediction: tensor([[[0.4160, 0.2197, 0.4171,  ..., 0.6421, 0.4400, 0.7535],\n",
      "         [0.4109, 0.2204, 0.4029,  ..., 0.6431, 0.4021, 0.7535],\n",
      "         [0.4160, 0.2217, 0.3997,  ..., 0.6518, 0.3920, 0.7723],\n",
      "         ...,\n",
      "         [0.4068, 0.2518, 0.4019,  ..., 0.6242, 0.4936, 0.6961],\n",
      "         [0.4033, 0.2510, 0.3881,  ..., 0.6009, 0.5246, 0.6856],\n",
      "         [0.3931, 0.2418, 0.3785,  ..., 0.5645, 0.5533, 0.6718]]],\n",
      "       device='mps:0', grad_fn=<PermuteBackward0>)\n",
      "Sign prediction shape: torch.Size([1, 368, 150])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_index = 3\n",
    "\n",
    "text = t2m_trans_dataset.texts[data_index]\n",
    "print(f'Text: \"{text}\"')\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel = vq_vae_dataset.get_full_sequences_by_idx(data_index)\n",
    "print(f\"Skeleton: {skel}\")\n",
    "print(f\"Skeleton shape: {skel.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel_indices = t2m_trans_dataset[data_index][1]\n",
    "print(f\"Skeleton indices: {skel_indices}\")\n",
    "print(f\"Skeleton indices shape: {skel_indices.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "skel_reconstructed = text_to_sign.vq_vae_model.decode_indices(skel_indices[:-1].unsqueeze(0))\n",
    "print(f\"Reconstructed skeleton: {skel_reconstructed}\")\n",
    "print(f\"Reconstructed skeleton shape: {skel_reconstructed.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "indices_prediction = text_to_sign.text_to_indices(text)\n",
    "print(f\"Indices prediction: {indices_prediction}\")\n",
    "print(f\"Indices prediction shape: {indices_prediction.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sign_prediction = text_to_sign.text_to_skels(text)\n",
    "print(f\"Sign prediction: {sign_prediction}\")\n",
    "print(f\"Sign prediction shape: {sign_prediction.shape}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = vq_vae_dataset.min_value\n",
    "max_value = vq_vae_dataset.max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to original values\n",
    "converted_skel = (skel * (max_value - min_value)) + min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = None\n",
    "if FOLDER_NAME:\n",
    "    import os\n",
    "\n",
    "    os.makedirs(FOLDER_NAME, exist_ok=True)\n",
    "else:\n",
    "    FOLDER_NAME = \".\"\n",
    "\n",
    "converted_skel_reconstructed = skel_reconstructed[0].cpu().detach()\n",
    "converted_skel_reconstructed = (converted_skel_reconstructed * (max_value - min_value)) + min_value\n",
    "plot_skeletons_video(converted_skel_reconstructed, FOLDER_NAME, \"reconstruction\", skel, 1, f\"{data_index}\")\n",
    "\n",
    "converted_sign_prediction = sign_prediction[0].cpu().detach()\n",
    "converted_sign_prediction = (converted_sign_prediction * (max_value - min_value)) + min_value\n",
    "# GPT is not work\n",
    "# plot_skeletons_video(converted_sign_prediction, FOLDER_NAME, \"prediction\", skel, 1, f\"{data_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
